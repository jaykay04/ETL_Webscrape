{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b99c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import os\n",
    "\n",
    "\n",
    "url = \"https://dev.to/latest\"\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "headers = {\"user-agent\": userAgent}\n",
    "page = requests.get(url, headers = headers)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e024cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_box = soup.find_all(\"div\", class_ = \"crayons-story__body\")\n",
    "\n",
    "links = []\n",
    "titles = []\n",
    "time_uploaded = []\n",
    "authors = []\n",
    "tags = []\n",
    "reading_times = []\n",
    "\n",
    "for box in blog_box:\n",
    "    #links\n",
    "    if box.find(\"h2\", class_ = \"crayons-story__title\") is not None:\n",
    "        link = box.find(\"h2\", class_ = \"crayons-story__title\").a\n",
    "        link = link[\"href\"]\n",
    "        links.append(link.strip())\n",
    "    else:\n",
    "        links.append(\"None\")\n",
    "        \n",
    "    #titles\n",
    "    if box.find(\"h2\", class_ = \"crayons-story__title\") is not None:\n",
    "        title = box.find(\"h2\", class_ = \"crayons-story__title\")\n",
    "        titles.append(title.text.replace(\"\\n\", \"\").strip())\n",
    "    else:\n",
    "        titles.append(\"None\")\n",
    "        \n",
    "    #time_uploaded\n",
    "    if box.find(\"time\", attrs = {\"datetime\": True}) is not None:\n",
    "        time_upload = box.find(\"time\", attrs = {\"datetime\": True})\n",
    "        time_upload = time_upload[\"datetime\"]\n",
    "        time_uploaded.append(time_upload)\n",
    "    else:\n",
    "        time_uploaded.append(\"None\")\n",
    "        \n",
    "    #authors\n",
    "    if box.find(\"a\", class_ = \"crayons-story__secondary fw-medium m:hidden\") is not None:\n",
    "        author = box.find(\"a\", class_ = \"crayons-story__secondary fw-medium m:hidden\")\n",
    "        authors.append(author.text.replace(\"\\n\", \"\").strip())\n",
    "    else:\n",
    "        authors.append(\"None\")\n",
    "        \n",
    "    #tags\n",
    "    if box.find(\"div\", class_ = \"crayons-story__tags\") is not None:\n",
    "        tag = box.find(\"div\", class_ = \"crayons-story__tags\")\n",
    "        tags.append(tag.text.replace(\"\\n\", \" \").strip())\n",
    "    else:\n",
    "        tags.append(\"None\")\n",
    "        \n",
    "    #reading_times\n",
    "    if box.find(\"div\",class_ = \"crayons-story__save\") is not None:\n",
    "        reading_time = box.find(\"div\",class_ = \"crayons-story__save\")\n",
    "        reading_times.append(reading_time.text.replace(\"\\n\", \"\").strip())\n",
    "    else:\n",
    "        reading_times.append(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "527226a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Link\": links,\n",
    "        \"Title\": titles,\n",
    "        \"Time_Uploaded\": time_uploaded,\n",
    "        \"Author\": authors,\n",
    "        \"Tag\": tags,\n",
    "        \"Reading_Time\": reading_times\n",
    "    }\n",
    ")\n",
    "\n",
    "blog_df = blog_df[blog_df[\"Link\"] != \"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de13c5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://dev.to/analaura/quando-foi-que-a-gente-parou-de-fazer-perguntas-45hc',\n",
       " 'https://dev.to/neuml/medical-rag-research-with-txtai-2b35',\n",
       " 'https://dev.to/learnspringtools/springbootfirststepsp1-56na',\n",
       " 'https://dev.to/learnspringtools/springbootfirststepsp2-4o52',\n",
       " 'https://dev.to/learnspringtools/springbootfirststepslib-54ki',\n",
       " 'https://dev.to/vaib/declarative-programming-sql-html-css-prolog-guide-nd0',\n",
       " 'https://dev.to/nabbisen/uiux-da-gui-mo-na-css-henodui-ying-3563',\n",
       " 'https://dev.to/mia_koring_962598adda4663/text-compressing-introduction-huffman-coding-in-swift-1o97',\n",
       " 'https://dev.to/fangmbeng/cyberguard-ai-107i',\n",
       " 'https://dev.to/sigurdurb/build-a-sanctions-check-in-10-minutes-with-sanctionsnap-api-4moh',\n",
       " 'https://dev.to/teruxz/devops-kak-po-uchiebniku-vozmozhno-li-ru-3jnb',\n",
       " 'https://dev.to/huetteldorf/announcing-pyseoa-ts-v020-more-power-more-control-49ji',\n",
       " 'https://dev.to/codanyks/stop-duct-taping-context-into-prompts-meet-the-mcp-server-hi5',\n",
       " 'https://dev.to/motchan/why-ai-keeps-butchering-your-designs-and-how-i-fixed-it-3f19',\n",
       " 'https://dev.to/surrealdb/databases-are-the-next-ai-frontier-156j',\n",
       " 'https://dev.to/pythonxi/building-a-web-based-excel-editor-a-comprehensive-guide-2p4b',\n",
       " 'https://dev.to/cxrtisxl/tlsnotary-flow-overview-17o0',\n",
       " 'https://dev.to/navashub/content-generator-from-youtube-video-id-3j6j',\n",
       " 'https://dev.to/teruxz/orghanizatsiia-iac-moduli-ru-4klc',\n",
       " 'https://dev.to/kireji/a-space-age-metaphor-for-software-development-3pie',\n",
       " 'https://dev.to/freedom_coder/cve-2023-20198-cisco-ios-xe-web-ui-privilege-escalation-vulnerability-3jk3',\n",
       " 'https://dev.to/vincenttommi/-3d6o',\n",
       " 'https://dev.to/msnmongare/how-long-does-it-take-to-understand-the-basics-of-a-new-programming-language-5748',\n",
       " 'https://dev.to/devopsfundamentals/big-data-fundamentals-big-data-tutorial-4l62',\n",
       " 'https://dev.to/vaib/building-ethical-ai-a-practical-guide-to-responsible-development-lad']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_df.Link.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2843c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = []\n",
    "article_link = []\n",
    "\n",
    "def get_full_content(url2):\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    headers = {\"user-agent\": userAgent}\n",
    "    page = requests.get(url2, headers = headers)\n",
    "\n",
    "    soup2 = BeautifulSoup(page.content, \"html.parser\")\n",
    "    #print(url2)\n",
    "\n",
    "\n",
    "\n",
    "    content = soup2.find(\"div\", class_ = \"crayons-article__main\")\n",
    "\n",
    "    paragraphs = content.find_all(\"p\")\n",
    "\n",
    "    contents = []\n",
    "\n",
    "    for x in paragraphs:\n",
    "        contents.append(x.text.replace(\"\\n\", \" \"))\n",
    "\n",
    "    full_content = \" \".join(contents)\n",
    "    article.append(full_content)\n",
    "    article_link.append(url2)\n",
    "    \n",
    "for i in blog_df.Link:\n",
    "    get_full_content(i)\n",
    "\n",
    "article_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Link\": article_link,\n",
    "        \"Article_Content\": article\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf1dad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = blog_df.merge(article_df, on = \"Link\", how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc5a557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langid\n",
      "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.3/1.9 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 0.5/1.9 MB 882.6 kB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.5/1.9 MB 882.6 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.8/1.9 MB 882.6 kB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 0.8/1.9 MB 882.6 kB/s eta 0:00:02\n",
      "     --------------------- ------------------ 1.0/1.9 MB 774.0 kB/s eta 0:00:02\n",
      "     --------------------- ------------------ 1.0/1.9 MB 774.0 kB/s eta 0:00:02\n",
      "     --------------------- ------------------ 1.0/1.9 MB 774.0 kB/s eta 0:00:02\n",
      "     --------------------- ------------------ 1.0/1.9 MB 774.0 kB/s eta 0:00:02\n",
      "     --------------------- ------------------ 1.0/1.9 MB 774.0 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 1.3/1.9 MB 504.6 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 1.3/1.9 MB 504.6 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 1.3/1.9 MB 504.6 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 1.3/1.9 MB 504.6 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 1.3/1.9 MB 504.6 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 1.3/1.9 MB 504.6 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 1.3/1.9 MB 504.6 kB/s eta 0:00:02\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.6/1.9 MB 358.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.8/1.9 MB 251.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.8/1.9 MB 251.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.8/1.9 MB 251.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.8/1.9 MB 251.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 236.6 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pycountry\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from langid) (2.2.6)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 246.6 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 0.5/6.3 MB 246.6 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 0.5/6.3 MB 246.6 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 0.5/6.3 MB 246.6 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 0.8/6.3 MB 272.8 kB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 0.8/6.3 MB 272.8 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 1.0/6.3 MB 324.7 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 1.0/6.3 MB 324.7 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 1.3/6.3 MB 360.8 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 1.3/6.3 MB 360.8 kB/s eta 0:00:14\n",
      "   --------- ------------------------------ 1.6/6.3 MB 399.5 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 1.6/6.3 MB 399.5 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 414.2 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 414.2 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 414.2 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 2.1/6.3 MB 422.4 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 2.1/6.3 MB 422.4 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 2.1/6.3 MB 422.4 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 2.4/6.3 MB 426.1 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 2.4/6.3 MB 426.1 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 432.7 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 432.7 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 432.7 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 2.9/6.3 MB 435.7 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 2.9/6.3 MB 435.7 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 3.1/6.3 MB 441.5 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 3.1/6.3 MB 441.5 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 3.1/6.3 MB 441.5 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 3.1/6.3 MB 441.5 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 3.4/6.3 MB 436.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 3.4/6.3 MB 436.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 3.4/6.3 MB 436.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 3.4/6.3 MB 436.7 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 413.1 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 413.1 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 413.1 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 413.1 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 413.1 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 3.9/6.3 MB 396.8 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 3.9/6.3 MB 396.8 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 3.9/6.3 MB 396.8 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 3.9/6.3 MB 396.8 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 3.9/6.3 MB 396.8 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 4.2/6.3 MB 379.0 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 4.2/6.3 MB 379.0 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 4.2/6.3 MB 379.0 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 4.2/6.3 MB 379.0 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 4.2/6.3 MB 379.0 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 4.2/6.3 MB 379.0 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 4.2/6.3 MB 379.0 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 4.2/6.3 MB 379.0 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 352.3 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 352.3 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 352.3 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 352.3 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 352.3 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 352.3 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 352.3 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 331.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 331.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 331.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 331.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 331.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 331.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 331.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 331.3 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 331.3 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 5.0/6.3 MB 308.5 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 5.2/6.3 MB 280.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.3 MB 280.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.3 MB 280.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.3 MB 280.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.3 MB 280.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.3 MB 280.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.3 MB 280.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.3 MB 280.9 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 5.2/6.3 MB 280.9 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 267.6 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 5.8/6.3 MB 250.8 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 6.0/6.3 MB 236.8 kB/s eta 0:00:02\n",
      "   ---------------------------------------  6.3/6.3 MB 224.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 224.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 223.5 kB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 0.5/1.5 MB 81.8 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 0.8/1.5 MB 95.1 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 1.0/1.5 MB 97.4 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 90.7 kB/s eta 0:00:03\n",
      "   ---------------------------------------- 1.5/1.5 MB 86.1 kB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: langid\n",
      "  Building wheel for langid (pyproject.toml): started\n",
      "  Building wheel for langid (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941226 sha256=63c6a4c7c446e65bdc88be0697f6982a668bd236a032cc136fb4d5e6ab88dbf2\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\23\\c8\\c6\\eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
      "Successfully built langid\n",
      "Installing collected packages: tqdm, regex, pycountry, langid, joblib, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/7 [tqdm]\n",
      "   ---------------------------------------- 0/7 [tqdm]\n",
      "   ----- ---------------------------------- 1/7 [regex]\n",
      "   ----------- ---------------------------- 2/7 [pycountry]\n",
      "   ----------- ---------------------------- 2/7 [pycountry]\n",
      "   ----------- ---------------------------- 2/7 [pycountry]\n",
      "   ----------------- ---------------------- 3/7 [langid]\n",
      "   ---------------------- ----------------- 4/7 [joblib]\n",
      "   ---------------------- ----------------- 4/7 [joblib]\n",
      "   ---------------------- ----------------- 4/7 [joblib]\n",
      "   ---------------------- ----------------- 4/7 [joblib]\n",
      "   ---------------------- ----------------- 4/7 [joblib]\n",
      "   ---------------------- ----------------- 4/7 [joblib]\n",
      "   ---------------------------- ----------- 5/7 [click]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------- ----- 6/7 [nltk]\n",
      "   ---------------------------------------- 7/7 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 joblib-1.5.1 langid-1.1.6 nltk-3.9.1 pycountry-24.6.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langid pycountry nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "034f482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the stopwords dataset\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "\n",
    "def count_words_without_stopwords(text):\n",
    "    if isinstance(text, (str, bytes)):\n",
    "        words = nltk.word_tokenize(str(text))\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        return len(filtered_words)\n",
    "    else:\n",
    "        0\n",
    "        \n",
    "merged_df['Word_Count'] = merged_df[\"Article_Content\"].apply(count_words_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40f815c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(record):\n",
    "    sentiment_scores = sent.polarity_scores(record)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "        \n",
    "    return compound_score, sentiment\n",
    "\n",
    "merged_df[['Compound_Score' ,'Sentiment']] = merged_df['Article_Content'].astype(str).apply(lambda x: pd.Series(get_sentiment(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7dda683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "import pycountry\n",
    "\n",
    "def detect_language(text):\n",
    "    # Convert Nan to an empty string\n",
    "    text = str(text) if pd.notna(text) else ''\n",
    "    \n",
    "    # Use langid to detect the language\n",
    "    lang, confidence = langid.classify(text)\n",
    "    return lang\n",
    "\n",
    "merged_df['Language'] = merged_df['Article_Content'].apply(detect_language)\n",
    "merged_df['Language'] = merged_df['Language'].map(lambda code: pycountry.languages.get(alpha_2 = code).name if pycountry.languages.get(alpha_2 = code) else code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e580811a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Time_Uploaded</th>\n",
       "      <th>Author</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Reading_Time</th>\n",
       "      <th>Article_Content</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Compound_Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://dev.to/neuml/medical-rag-research-with...</td>\n",
       "      <td>Medical RAG Research with txtai</td>\n",
       "      <td>2025-06-23T21:23:43Z</td>\n",
       "      <td>David Mezzetti</td>\n",
       "      <td>#ai #llm #rag #vectordatabase</td>\n",
       "      <td>7</td>\n",
       "      <td>txtai is an all-in-one AI framework for semant...</td>\n",
       "      <td>458</td>\n",
       "      <td>0.9858</td>\n",
       "      <td>Positive</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://dev.to/learnspringtools/springbootfirs...</td>\n",
       "      <td>springboot.firststeps(P1)</td>\n",
       "      <td>2025-06-23T21:06:55Z</td>\n",
       "      <td>olver team</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>personaController.java  @DateTimeFormat(patte...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://dev.to/vaib/declarative-programming-sq...</td>\n",
       "      <td>Declarative Programming: SQL, HTML, CSS, Prolo...</td>\n",
       "      <td>2025-06-23T21:02:36Z</td>\n",
       "      <td>Vaiber</td>\n",
       "      <td>#programming #declarative #sql #prolog</td>\n",
       "      <td>7</td>\n",
       "      <td>Welcome, fellow developers and curious minds! ...</td>\n",
       "      <td>666</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>Positive</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://dev.to/mia_koring_962598adda4663/text-...</td>\n",
       "      <td>Text Compressing Introduction - Huffman Coding...</td>\n",
       "      <td>2025-06-23T20:49:39Z</td>\n",
       "      <td>Mia Koring</td>\n",
       "      <td>#computerscience #algorithms #swift #beginners</td>\n",
       "      <td>1</td>\n",
       "      <td>Ever wondered how file compression actually wo...</td>\n",
       "      <td>111</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Positive</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://dev.to/fangmbeng/cyberguard-ai-107i</td>\n",
       "      <td>CyberGuard AI</td>\n",
       "      <td>2025-06-23T20:46:48Z</td>\n",
       "      <td>Brandon Atonte</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>The cybersecurity landscape is facing an unpre...</td>\n",
       "      <td>717</td>\n",
       "      <td>-0.9781</td>\n",
       "      <td>Negative</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  \\\n",
       "0  https://dev.to/neuml/medical-rag-research-with...   \n",
       "1  https://dev.to/learnspringtools/springbootfirs...   \n",
       "2  https://dev.to/vaib/declarative-programming-sq...   \n",
       "3  https://dev.to/mia_koring_962598adda4663/text-...   \n",
       "4        https://dev.to/fangmbeng/cyberguard-ai-107i   \n",
       "\n",
       "                                               Title         Time_Uploaded  \\\n",
       "0                    Medical RAG Research with txtai  2025-06-23T21:23:43Z   \n",
       "1                          springboot.firststeps(P1)  2025-06-23T21:06:55Z   \n",
       "2  Declarative Programming: SQL, HTML, CSS, Prolo...  2025-06-23T21:02:36Z   \n",
       "3  Text Compressing Introduction - Huffman Coding...  2025-06-23T20:49:39Z   \n",
       "4                                      CyberGuard AI  2025-06-23T20:46:48Z   \n",
       "\n",
       "           Author                                             Tag  \\\n",
       "0  David Mezzetti                   #ai #llm #rag #vectordatabase   \n",
       "1      olver team                                                   \n",
       "2          Vaiber          #programming #declarative #sql #prolog   \n",
       "3      Mia Koring  #computerscience #algorithms #swift #beginners   \n",
       "4  Brandon Atonte                                                   \n",
       "\n",
       "   Reading_Time                                    Article_Content  \\\n",
       "0             7  txtai is an all-in-one AI framework for semant...   \n",
       "1             3   personaController.java  @DateTimeFormat(patte...   \n",
       "2             7  Welcome, fellow developers and curious minds! ...   \n",
       "3             1  Ever wondered how file compression actually wo...   \n",
       "4             4  The cybersecurity landscape is facing an unpre...   \n",
       "\n",
       "   Word_Count  Compound_Score Sentiment Language  \n",
       "0         458          0.9858  Positive  English  \n",
       "1          22          0.0000   Neutral  English  \n",
       "2         666          0.9989  Positive  English  \n",
       "3         111          0.9603  Positive  English  \n",
       "4         717         -0.9781  Negative  English  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = merged_df[merged_df['Language'] == 'English'].reset_index(drop = True)\n",
    "filtered_df['Reading_Time'] = filtered_df['Reading_Time'].str.replace(' min read', '', regex=False).str.strip().astype(int)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30d893db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.9.10)\n"
     ]
    }
   ],
   "source": [
    "# CREATE TABLE IF NOT EXISTS articles(\n",
    "# Link TEXT,\n",
    "# Title TEXT,\n",
    "# Time_Uploaded TIMESTAMP,\n",
    "# Author TEXT,\n",
    "# Tag TEXT,\n",
    "# Reading_Time INTEGER,\n",
    "# Article_Content TEXT,\n",
    "# Word_Count INTEGER,\n",
    "# Compound_Score NUMERIC,\n",
    "# Sentiment TEXT,\n",
    "# Language TEXT\n",
    "# );\n",
    "\n",
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_params = {\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # SQL Insert Query\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO articles (Link, Title, Time_Uploaded, Author, Tag, Reading_Time, Article_Content, Word_Count, Compound_Score, Sentiment, Language)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s,%s, %s, %s, %s)\n",
    "    ON CONFLICT (Link) DO NOTHING;  -- Avoids duplicate primary key errors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Insert DataFrame records one by one\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        cursor.execute(insert_query, (\n",
    "            row['Link'], row['Title'], row['Time_Uploaded'],  row['Author'], row['Tag'], row['Reading_Time'],\n",
    "            row['Article_Content'],row['Word_Count'],row['Compound_Score'],row['Sentiment'],row['Language']\n",
    "        ))\n",
    "\n",
    "    # Commit and close\n",
    "    conn.commit()\n",
    "    print(\"Data inserted successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    if conn:\n",
    "        cursor.close()\n",
    "        conn.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15566e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
